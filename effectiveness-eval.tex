\section{Effectiveness Evaluation}
\label{sec:effectiveness-eval}

\subsection{Data Collection}\label{sec:effectiveness-data}

\input{tables/data-stats}

To enable the effectiveness evaluation, we considered six popular Java libraries that developers typically make use~of: \code{android}, \code{gwt}, \code{hibernate}, \code{jdk}, \code{joda-time}, and \code{xstream}. This set has been used in prior works on FQN resolution~\cite{icse18, snr-icse22, prompt-ase22}. Among these, StatType\cite{icse18} and SnR\cite{snr-icse22} leverage projects that utilize these libraries. A limitation with this way is that not all library API elements are covered in their dataset. In contrast, we download these large-scale repositories from GitHub and use their source code itself to build our dataset.

We used Eclipse JDT to parse each project’s source code and resolve
all the FQNs.
%Next, we traversed the ASTs to collect all the methods.
Overall, these libraries contain 198,231 methods including 9,764 from
\code{android}, 62,782 from \code{gwt}, 105,734 from
\code{hibernate-orm}, 5,235 from \code{jdk}, 9,811 from
\code{joda-time}, and 4,905 from \code{xstream}. Next, we randomly
split these in a 80\%-10\%-10\% ratio, each for training, validation,
and testing, respectively. Finally, we applied the TILE algorithm to
identify the type inference locations and build the
$\langle$\blank-code, FQN$\rangle$ pairs, discarding the methods with
no such pairs. In general, we have 125,163 instances carrying 510,664
FQNs. Among these, 58,543 API elements carry unique FQNs, and 21,660
carry unique FQN prefixes (i.e., unresolved part of the FQN). We
report the fine-grained library/split-level data statistics in
Table~\ref{tab:data-stats}.

\subsection{Procedure and Evaluation Metrics}\label{sec:effectiveness-eval-proc}
We design this experiment to test the effectiveness of \tool in resolving FQNs in complete code. For all experiments, we train the same CLM, i.e., GPT-2 “small” (containing 124M parameters) within the ILM framework.

%\noindent\underline{\textit{Baselines}}: Due to the formulation of our approach as a fill-in-the-blank task, we can establish a direct comparison with the following: (a) off-the-shelf CodeBERT model (MLM\textsubscript{\textit{base}})~\cite{codebert}, (b) prompt-tuned CodeBERT model (MLM\textsubscript{\textit{FIB}})~\cite{prompt-ase22}. Besides, Huang {\em et al.}~\cite{prompt-ase22} reported that reserving 30\% of the data for the prompt-tuning phase gives them stable model performance. Accordingly, for MLM\textsubscript{\textit{FIB}}, we reserve 30\% of our training data for prompt-tuning, and remaining 70\% for fine-tuning.

\noindent\underline{\textit{Baselines}}: Due to the formulation of our approach as a fill-in-the-blank task, we can establish a direct comparison with prompt-tuned CodeBERT model (MLM\textsubscript{\textit{FIB}})~\cite{prompt-ase22}. Besides, Huang {\em et al.}~\cite{prompt-ase22} reported that reserving 30\% of the data for the prompt-tuning phase gives them stable model performance. Accordingly, for MLM\textsubscript{\textit{FIB}}, we reserve 30\% of our training data for prompt-tuning, and remaining 70\% for fine-tuning.

\noindent\underline{\textit{Data Preparation for Baselines}}: We followed the FQN prompt generation steps detailed in \cite{prompt-ase22} to build the training data for both prompt-tuning and fine-tuning phases in MLM\textsubscript{\textit{FIB}}. Due to the code fragmentation step in MLM\textsubscript{\textit{FIB}}, each training instance with $k$ type inference points in our dataset (i.e., at the method-level) corresponds to $k$ prompts/training instances for this baseline. Note that the whole FQNs in a prompt are masked during the prompt-tuning phase, while the FQN prefixes are masked in the fine-tuning phase.

\noindent\underline{\textit{FQN Resolution with Baselines}}: 
%In MLM\textsubscript{\textit{base}}, there is no additional training required since we use CodeBERT off-the-shelf. 
In MLM\textsubscript{\textit{FIB}}, CodeBERT is first prompt-tuned, then fine-tuned to enable FQN resolution. An MLM is not designed to handle variable lengths of missing spans, and needs to know the length of the FQN prefix (i.e., the number of sub-tokens it corresponds to) beforehand. However, this is presumably not available during the inference phase. Huang {\em et al.}~\cite{prompt-ase22} address this issue by producing multiple prompts with the lengths of FQNs ranging from the minimum mask span (i.e., 2) and the maximum mask span (i.e., 52), and select the FQN prefix prediction with the highest average probability. We followed suit to establish the inference process.% with both baselines.

\noindent\underline{\textit{Evaluation Metrics}}: We use the following metrics to assess the model performance: (a) \textit{Exact Match Accuracy} ({\em Accuracy\textsubscript{EM}}), a stricter version of accuracy which ensures that the predicted FQN is exactly the same as the ground-truth FQN; (b) \textit{ROUGE-L}\footnote{Given ground-truth FQN $X$ and predicted FQN $Y$ of lengths $x$ and $y$, respectively: Recall, $R_{LCS}=\frac{LCS(X, Y)}{x}$, Precision, $P_{LCS}=\frac{LCS(X, Y)}{y}$, and F-measure, $F_{LCS}=\frac{2.R_{LCS}.P_{LCS}}{R_{LCS}+P_{LCS}}$.}, which compares the predicted FQN and the ground-truth FQN based on the longest common subsequence (LCS); and (c) \textit{BLEU-2}, which compares the similarity between the predicted FQN and the ground-truth FQN based on bigrams in the subword tokens.
The rationale behind adopting both \textit{ROUGE-L} and \textit{BLEU-2} is that the latter is order-agnostic beyond the $n$-gram size, i.e., the comparison is made regardless of the sub-token order. In contrast, since \textit{ROUGE-L} is based on LCS, it helps assess the sub-token order. Besides, the minimum number of sub-tokens in the FQN prefixes in our dataset is 2. In the case that BLEU finds zero counts of a higher $n$-gram overlaps, independently of how many $n$-gram overlaps of lower order it contains, it evaluates it to zero. Thus, we chose bigrams as the highest $n$-gram order in \textit{BLEU}.

\input{rq1-results}
