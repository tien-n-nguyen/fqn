\section{Practicality Evaluation}
\label{sec:eval}

In this experiment, we seek to establish \tool's efficacy in a more practical setting, i.e., in resolving FQNs for real-world (incomplete) code snippets from StackOverflow.

\subsection{Experiment Methodology}
Phan {\em et al.}~\cite{icse18} collected a benchmark dataset of 268 code snippets from StackOverflow to represent how developers use Java libraries in practice. To establish the ground-truth FQN annotations for these code snippets, they manually added the required libraries and made them compilable. These code snippets are relevant to the six libraries listed in Section~\ref{sec:effectiveness-data}. For brevity, let us refer to this dataset as StatType-SO. Among these, 
%the code snippets utilizing \code{jdk} and \code{android} libraries reference a wider range of APIs. 
we test the performance on code snippets utilizing \code{gwt}, \code{hibernate-orm}, and \code{xstream} in this experiment.
%Overall, StatType-SO has \_\_ API elements, of which, \_\_ are unique.
%\subsection{Procedure \& Evaluation Metrics}
To retrieve our data instances for this experiment, first, we employ the TILE algorithm (see Section~\ref{sec:tile}) to identify all type inference points across the methods in the StatType-SO dataset. Next, we utilize the constructed $\langle$\blank-code, FQN$\rangle$ pairs to create infilling examples consistent with Section~\ref{sec:effectiveness-eval-proc}. Finally, we leverage the trained \tool model to predict the missing FQNs in these infilling examples.
% \underline{\textit{Baseline}}: The state-of-the-art approach,
% MLM\textsubscript{\textit{FIB}}~\cite{prompt-ase22}, for FQN
% resolution outperforms previous works~\cite{coster-ase19, snr-icse22}
% on the benchmark StatType-SO dataset. Therefore, we chose
% MLM\textsubscript{\textit{FIB}} as the only baseline for this
% experiment. To facilitate the comparison, we construct data instances
% as in Section~\ref{sec:effectiveness-eval-proc}, and leverage the
% trained MLM\textsubscript{\textit{FIB}} to infer the FQNs.
%\underline{\textit{Evaluation Metrics}}:
We adopt the same evaluation metrics as in Section~\ref{sec:effectiveness-eval-proc}, i.e., Accuracy\textsubscript{\textit{EM}}, ROUGE-L, and BLEU-2.

\input{rq4-results}


%Alternatively, the StatType-SO dataset contains an increased number of ex- ternal FQNs (76.4\%) (col.E-Fs) w.r.t COSTER-SO. Such a behaviour is also reflected in the number of unique external FQNs (col.UE-Fs) with only 10 for COSTER-SO and 128 for StatType-SO. To further increase the evaluation setup, we created another dataset (RESICO-SO) that replicates the distribution of FQNs in the StatType-SO dataset and possibly improves the number of unique external FQNs compared to previous datasets. This new dataset represents the third dataset considered to verify the generalisability of the results.

