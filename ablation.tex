\section{Ablation Study (RQ4)}
\label{sec:ablation}

In Section~\ref{sec:key}, we hypothesized that a wider {\em dependency
  context} can provide crucial hints towards overcoming name ambiguity
and identifying the API elements better. Accordingly, we input the
code of the entire method to train {\tool}. Let us denote such a
dataset by $D_w$. In contrast,
MLM\textsubscript{\textit{FIB}}~\cite{prompt-ase22} leverages a
narrower surrounding context (two lines before and after the statement
containing the API element). Let us denote this dataset by $D_n$. The
details for building $D_w$ and $D_n$ were described in
Section~\ref{sec:effectiveness-eval-proc}.

The goal of this experiment is to gauge the impact that the wider
dependency context has on FQN resolution. Thus, we conducted an
ablation on the amount of contextual information, and created two
baselines: (a) \tool w/ dependency context, i.e., when trained on
$D_w$, (b) \tool w/o dependency context, i.e., trained on $D_n$. We
evaluate both ablation baselines against the test set in
Section~\ref{sec:effectiveness-data}.

\input{rq2-results}
